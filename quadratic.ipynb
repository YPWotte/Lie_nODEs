{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchdyn.numerics.odeint import odeint_hybrid\n",
    "from torchdyn.numerics.solvers import DormandPrince45\n",
    "import torchdyn.numerics.sensitivity\n",
    "import attr\n",
    "\n",
    "## lietorch:\n",
    "import sys; sys.path.append('../')\n",
    "import lie_torch as lie\n",
    "import math\n",
    "\n",
    "from functorch import vmap\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Quadratic Shaping Only </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Definition: Mixed Dynamics on SE3 with NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SE3_Dynamics, SE3_Quadratic, AugmentedSE3, PosDefSym, PosDefTriv, PosDefSym_Small\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity / Adjoint Method: v1\n",
    "\n",
    "This only needs the final condition of the forward dynamics and adjoint dynamics, then it computes both state and adjoint state from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensitivity import _gather_odefunc_hybrid_adjoint_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learners: Training step, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners import EnergyShapingLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event definition: Chart Switching on SE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from pytorch-implicit/exampels/network/simulate_tcp.ipynb, and Paper: Neural Hybrid Automata: Learning Dynamics withMultiple Modes and Stochastic Transitions (M.Poli, 2021)\n",
    "\n",
    "@attr.s\n",
    "class EventCallback(nn.Module):\n",
    "    def __attrs_post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def check_event(self, t, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def jump_map(self, t, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_jump(self, t, x, ev):\n",
    "        raise NotImplementedError\n",
    "\n",
    "@attr.s\n",
    "class ChartSwitch(EventCallback):       \n",
    "    def check_event(self, t, xi): \n",
    "        # works for collection of states\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        wi = qi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(wi,wi)) > math.pi*3/4).bool() \n",
    "        return ev\n",
    "\n",
    "    def jump_map(self, t, xi):\n",
    "        #xi = torch.squeeze(xi)\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = torch.unsqueeze(lie.bestChart(H),0)\n",
    "        qj = lie.chart_trans(qi, i, j)\n",
    "        return torch.cat((qj, P,j,rem), -1) #torch.unsqueeze(,0)\n",
    "    \n",
    "    def batch_jump(self, t, xi, ev):\n",
    "        xi[ev,:] = vmap(self.jump_map)(t[ev],xi[ev,:])\n",
    "        return xi\n",
    "        \n",
    "    \n",
    "@attr.s\n",
    "class ChartSwitchAugmented(EventCallback):\n",
    "    des_props = None\n",
    "    # Expects x of type: z[:6] = qi, z[6:12] = P, z[12] = i, z[13:25] = λi, z[25:] = μ. This is used for the system augmented with co-state-dynamics for adjoint gradient method\n",
    "    def check_event(self, t, z): \n",
    "        xi, i, λi, rem = self.to_input(z)\n",
    "        w = xi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(w,w)) > math.pi*3/4).bool()  \n",
    "        return ev\n",
    "\n",
    "    def jump_map(self, t, z):\n",
    "        xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "        qi = xi[...,:6]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = lie.bestChart(H)\n",
    "        xj = lie.chart_trans_mix(xi, i, j)\n",
    "        λj = lie.chart_trans_mix_Co(xi, λi, i, j)\n",
    "        return torch.cat((xj, torch.unsqueeze(j,-1), λj, rem), -1)\n",
    "    \n",
    "    def batch_jump(self, t, z, ev):\n",
    "        xi, i, λii, rem = self.to_input(z)\n",
    "        z = torch.cat((xi,torch.unsqueeze(i,-1),λii),-1)\n",
    "        z[ev,:] = vmap(self.jump_map)(t[:xi.shape[0]][ev],z[ev,:])\n",
    "        xi, i, λii = z[...,:12], z[...,12], z[...,13:26] \n",
    "        return self.to_output(xi, i, λii, rem)\n",
    "    \n",
    "    def to_input(self, z):\n",
    "        if (self.des_props!=None):\n",
    "            numels,shapes = tuple(self.des_props)\n",
    "            xii_nel, λi_nel = tuple(numels)\n",
    "            xii_shp, λi_shp = tuple(shapes)\n",
    "            xii, λii, rem = z[:xii_nel], z[xii_nel:xii_nel+λi_nel], z[xii_nel+λi_nel:]\n",
    "            xii, λii = xii.reshape(xii_shp), λii.reshape(λi_shp)\n",
    "            xi, i = xii[...,:12], torch.unsqueeze(xii[...,12],-1)\n",
    "            return xi, i, λii, rem\n",
    "        else:\n",
    "            xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "            return xi, i, λi, rem\n",
    "    \n",
    "    def to_output(self, xj, j, λj, rem):\n",
    "        if (self.des_props!= None):\n",
    "            xjj = torch.cat((xj,torch.unsqueeze(j,-1)),-1)\n",
    "            z = torch.cat((xjj.flatten(),λj.flatten(),rem))\n",
    "        else:\n",
    "            z = torch.cat((xj, torch.unsqueeze(j,-1), λj, rem), -1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of Dynamics, Definition of Loss-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from latent-energy-shaping-main/notebooks/optimal_energy_shaping.ipynb\n",
    "\n",
    "I = torch.diag(torch.tensor((0.01,0.01,0.01,1,1,1))).to(device) ; # Inertia Tensor\n",
    "\n",
    "from models import IntegralLoss_Quadratic\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definition of NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 components of potential function, one per chart:\n",
    "# nh = 32\n",
    "\n",
    "Kd = nn.Sequential(nn.Linear(1, 6),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh, 21),PosDefSym())\n",
    "G0 = nn.Sequential(nn.Linear(1, 3),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh,6),PosDefSym_Small())\n",
    "Gt = nn.Sequential(nn.Linear(1, 3),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh,6),PosDefSym_Small())\n",
    "\n",
    "### Initialize Parameters: \n",
    "\n",
    "for net in (Kd,G0,Gt):\n",
    "    for p in net.parameters():torch.nn.init.zeros_(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of Chart-Switches, Prior- \\& Target Distribution, and Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prior and Target Distribution\n",
    "\n",
    "from utils import prior_dist_SE3, target_dist_SE3, multinormal_target_dist\n",
    "\n",
    "th_max = torch.tensor(math.pi).to(device); d_max = torch.tensor(1).to(device); pw_max = torch.tensor(0.03).to(device); pv_max = torch.tensor(1).to(device); ch_min = torch.tensor(0).to(device); ch_max = torch.tensor(0).to(device); \n",
    "prior = prior_dist_SE3(th_max,d_max,pw_max,pv_max,ch_min,ch_max,device)\n",
    "\n",
    "H_target = torch.eye(4).to(device); sigma_th = torch.tensor(1).to(device); sigma_d = torch.tensor(1).to(device); sigma_pw = torch.tensor(1e-1).to(device); sigma_p = torch.tensor(1e-1).to(device);\n",
    "target = target_dist_SE3(H_target,sigma_th,sigma_d,sigma_pw,sigma_p,device)\n",
    "\n",
    "### Callback:\n",
    "callbacks = [ChartSwitch()]\n",
    "jspan = 10 # maximum number of chart switches per iteration (if this many happen, something is wrong anyhow)\n",
    "\n",
    "callbacks_adjoint = [ChartSwitchAugmented()]\n",
    "jspan_adjoint = 10\n",
    "\n",
    "### Initialize Dynamics of Quadratic Controller\n",
    "\n",
    "t_span = torch.linspace(0, 1, 30).to(device) \n",
    "\n",
    "f_Quadratic = SE3_Quadratic(I,Kd,G0,Gt,H_target).to(device) # torch.load('f_Quadratic.pt')\n",
    "\n",
    "aug_f_Quadratic = AugmentedSE3(f_Quadratic, IntegralLoss_Quadratic(f_Quadratic)).to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop: Quadratic Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mypwotte\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.13"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yannik/Documents/Python/PhD/Thesis-Code-WIP/v2/wandb/run-20220414_130100-3nebkt4v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ypwotte/quadratic-shaping-SE3/runs/3nebkt4v\" target=\"_blank\">quadratic-controller</a></strong> to <a href=\"https://wandb.ai/ypwotte/quadratic-shaping-SE3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | NeuralODE_Hybrid | 24    \n",
      "1 | aug_model | NeuralODE_Hybrid | 24    \n",
      "-----------------------------------------------\n",
      "24        Trainable params\n",
      "0         Non-trainable params\n",
      "24        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1938: PossibleUserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff426250d2641b6aef870692668d8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Aborted!\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "from HybridODE import NeuralODE_Hybrid\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "solver = 'dopri5'\n",
    "atol, rtol, atol_adjoint, rtol_adjoint = 1e-4,1e-4,1e-4,1e-4\n",
    "dt_min, dt_min_adjoint = 0, 0\n",
    "\n",
    "model_Q = NeuralODE_Hybrid(f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, dt_min, atol_adjoint, rtol_adjoint, dt_min_adjoint, IntegralLoss_Quadratic(f_Quadratic), sensitivity = 'hybrid_adjoint_full').to(device) \n",
    "aug_model_Q = NeuralODE_Hybrid(aug_f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, dt_min, atol_adjoint, rtol_adjoint, dt_min_adjoint, sensitivity = 'hybrid_adjoint_full').to(device) \n",
    "#model_Q = NeuralODE_Hybrid(f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint, IntegralLoss_Quadratic(f_Quadratic)).to(device) \n",
    "#aug_model_Q = NeuralODE_Hybrid(aug_f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint).to(device) \n",
    "learn_Q = EnergyShapingLearner(model_Q, t_span, prior, target, aug_model_Q).to(device) \n",
    "learn_Q.lr = 1e-2\n",
    "learn_Q.batch_size = 100\n",
    "logger_Q = WandbLogger(project='quadratic-shaping-SE3',name='quadratic-controller')\n",
    "trainer = pl.Trainer(max_epochs=1000, logger=logger_Q,gpus = torch.cuda.device_count())#\n",
    "trainer.fit(learn_Q)\n",
    "torch.save(f_Quadratic,'f_Quadratic.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop: Optimal Potential Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Plots and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
