{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/Software/anaconda3/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchdyn.numerics.odeint import odeint_hybrid\n",
    "from torchdyn.numerics.solvers import DormandPrince45\n",
    "import torchdyn.numerics.sensitivity\n",
    "import attr\n",
    "\n",
    "## lietorch:\n",
    "import sys; sys.path.append('../')\n",
    "import lie_torch as lie\n",
    "import math\n",
    "\n",
    "from functorch import vmap\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Quadratic and General Potential Shaping in one</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Definition: Mixed Dynamics on SE3 with NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SE3_Dynamics, SE3_Quadratic, AugmentedSE3, PosDefSym, PosDefTriv, PosDefSym_Small\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity / Adjoint Method: v1\n",
    "\n",
    "This only needs the final condition of the forward dynamics and adjoint dynamics, then it computes both state and adjoint state from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensitivity import _gather_odefunc_hybrid_adjoint_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity / Adjoint Method: v2\n",
    "\n",
    "Requires full state trajectory of forward dynamics, but does not compute forward dynamics again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensitivity import _gather_odefunc_hybrid_adjoint_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learners: Training step, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners import EnergyShapingLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event definition: Chart Switching on SE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from pytorch-implicit/exampels/network/simulate_tcp.ipynb, and Paper: Neural Hybrid Automata: Learning Dynamics withMultiple Modes and Stochastic Transitions (M.Poli, 2021)\n",
    "\n",
    "@attr.s\n",
    "class EventCallback(nn.Module):\n",
    "    def __attrs_post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def check_event(self, t, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def jump_map(self, t, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_jump(self, t, x, ev):\n",
    "        raise NotImplementedError\n",
    "\n",
    "@attr.s\n",
    "class ChartSwitch(EventCallback):       \n",
    "    def check_event(self, t, xi): \n",
    "        # works for collection of states\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        wi = qi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(wi,wi)) > math.pi*3/4).bool() \n",
    "        return ev\n",
    "\n",
    "    def jump_map(self, t, xi):\n",
    "        #xi = torch.squeeze(xi)\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = torch.unsqueeze(lie.bestChart(H),0)\n",
    "        qj = lie.chart_trans(qi, i, j)\n",
    "        return torch.cat((qj, P,j,rem), -1) #torch.unsqueeze(,0)\n",
    "    \n",
    "    def batch_jump(self, t, xi, ev):\n",
    "        xi[ev,:] = vmap(self.jump_map)(t[ev],xi[ev,:])\n",
    "        return xi\n",
    "        \n",
    "    \n",
    "@attr.s\n",
    "class ChartSwitchAugmented(EventCallback):\n",
    "    des_props = None\n",
    "    # Expects x of type: z[:6] = qi, z[6:12] = P, z[12] = i, z[13:25] = λi, z[25:] = μ. This is used for the system augmented with co-state-dynamics for adjoint gradient method\n",
    "    def check_event(self, t, z): \n",
    "        xi, i, λi, rem = self.to_input(z)\n",
    "        w = xi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(w,w)) > math.pi*3/4).bool()  \n",
    "        return ev\n",
    "\n",
    "    def jump_map(self, t, z):\n",
    "        xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "        qi = xi[...,:6]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = lie.bestChart(H)\n",
    "        xj = lie.chart_trans_mix(xi, i, j)\n",
    "        λj = lie.chart_trans_mix_Co(xi, λi, i, j)\n",
    "        return torch.cat((xj, torch.unsqueeze(j,-1), λj, rem), -1)\n",
    "    \n",
    "    def batch_jump(self, t, z, ev):\n",
    "        xi, i, λii, rem = self.to_input(z)\n",
    "        z = torch.cat((xi,i,λii),-1)\n",
    "        z[ev,:] = vmap(self.jump_map)(t[:xi.shape[0]][ev],z[ev,:])\n",
    "        xi, i, λii = z[...,:12], z[...,12], z[...,13:26] \n",
    "        return self.to_output(xi, i, λii, rem)\n",
    "    \n",
    "    def to_input(self, z):\n",
    "        if (self.des_props!=None):\n",
    "            numels,shapes = tuple(self.des_props)\n",
    "            xii_nel, λi_nel = tuple(numels)\n",
    "            xii_shp, λi_shp = tuple(shapes)\n",
    "            xii, λii, rem = z[:xii_nel], z[xii_nel:xii_nel+λi_nel], z[xii_nel+λi_nel:]\n",
    "            xii, λii = xii.reshape(xii_shp), λii.reshape(λi_shp)\n",
    "            xi, i = xii[...,:12], torch.unsqueeze(xii[...,12],-1)\n",
    "            return xi, i, λii, rem\n",
    "        else:\n",
    "            xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "            return xi, i, λi, rem\n",
    "    \n",
    "    def to_output(self, xj, j, λj, rem):\n",
    "        if (self.des_props!= None):\n",
    "            xjj = torch.cat((xj,torch.unsqueeze(j,-1)),-1)\n",
    "            z = torch.cat((xjj.flatten(),λj.flatten(),rem))\n",
    "        else:\n",
    "            z = torch.cat((xj, j, λj, rem), -1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of Dynamics, Definition of Loss-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from latent-energy-shaping-main/notebooks/optimal_energy_shaping.ipynb\n",
    "\n",
    "I = torch.diag(torch.tensor((0.01,0.01,0.01,1,1,1))); # Inertia Tensor\n",
    "\n",
    "from models import IntegralLoss, IntegralLoss_Quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definition of NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 components of potential function, one per chart:\n",
    "nh = 32\n",
    "V0 = nn.Sequential(nn.Linear(6, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1))\n",
    "V1 = nn.Sequential(nn.Linear(6, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1))\n",
    "V2 = nn.Sequential(nn.Linear(6, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1))\n",
    "V3 = nn.Sequential(nn.Linear(6, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1))\n",
    "\n",
    "V = (V0,V1,V2,V3)\n",
    "\n",
    "### Likewise for Damping injection:\n",
    "B0 = nn.Sequential(nn.Linear(12, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 6))\n",
    "B1 = nn.Sequential(nn.Linear(12, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 6))\n",
    "B2 = nn.Sequential(nn.Linear(12, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 6))\n",
    "B3 = nn.Sequential(nn.Linear(12, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 6))\n",
    "BFin = PosDefTriv()\n",
    "\n",
    "B = (B0,B1,B2,B3,BFin)\n",
    "\n",
    "### \n",
    "\n",
    "Kd = nn.Sequential(nn.Linear(1, 6),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh, 21),PosDefSym())\n",
    "G0 = nn.Sequential(nn.Linear(1, 3),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh,6),PosDefSym_Small())\n",
    "Kt = nn.Sequential(nn.Linear(1, 3),PosDefTriv()) #nn.Sequential(nn.Linear(1, nh),nn.Softplus(), nn.Linear(nh,nh), nn.Softplus(), nn.Linear(nh,6),PosDefSym_Small())\n",
    "\n",
    "### Initialize Parameters: \n",
    "\n",
    "for Vi in V: \n",
    "    for p in Vi.parameters(): torch.nn.init.zeros_(p)\n",
    "for i in range(4): \n",
    "    for p in B[i].parameters(): torch.nn.init.zeros_(p)\n",
    "\n",
    "for net in (Kd,G0,Kt):\n",
    "    for p in net.parameters():torch.nn.init.zeros_(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of Chart-Switches, Prior- \\& Target Distribution, and Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prior and Target Distribution\n",
    "\n",
    "from utils import prior_dist_SE3, target_dist_SE3, multinormal_target_dist\n",
    "\n",
    "th_max = torch.tensor(math.pi).to(device); d_max = torch.tensor(1).to(device); pw_max = torch.tensor(0.03).to(device); pv_max = torch.tensor(1).to(device); ch_min = torch.tensor(0).to(device); ch_max = torch.tensor(0).to(device); \n",
    "prior = prior_dist_SE3(th_max,d_max,pw_max,pv_max,ch_min,ch_max,device)\n",
    "\n",
    "H_target = torch.eye(4).to(device); sigma_th = torch.tensor(0.01).to(device); sigma_d = torch.tensor(0.01).to(device); sigma_pw = torch.tensor(1e-5).to(device); sigma_p = torch.tensor(1e-3).to(device);\n",
    "target = target_dist_SE3(H_target,sigma_th,sigma_d,sigma_pw,sigma_p,device)\n",
    "\n",
    "### Callback:\n",
    "#theta_limit = math.pi*3/4; # Chart-switching limit\n",
    "callbacks = [ChartSwitch()]\n",
    "jspan = 10 # maximum number of chart switches per iteration (if this many happen, something is wrong anyhow)\n",
    "\n",
    "callbacks_adjoint = [ChartSwitchAugmented()]\n",
    "jspan_adjoint = 10\n",
    "\n",
    "### Initialize Dynamics \n",
    "f = SE3_Dynamics(I,B,V).to(device) # (I,B,V) = torch.load('IBV_fShaping.pt')\n",
    "\n",
    "aug_f = AugmentedSE3(f, IntegralLoss(f)).to(device) \n",
    "\n",
    "t_span = torch.linspace(0, 1, 30) \n",
    "\n",
    "### Dynamics of Quadratic Controller\n",
    "f_Quadratic = SE3_Quadratic(I,Kd,G0,Kt,H_target).to(device) # torch.load('f_Quadratic.pt')\n",
    "\n",
    "aug_f_Quadratic = AugmentedSE3(f_Quadratic, IntegralLoss_Quadratic(f_Quadratic)).to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainign Loop: Quadratic Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | NeuralODE_Hybrid | 24    \n",
      "1 | aug_model | NeuralODE_Hybrid | 24    \n",
      "-----------------------------------------------\n",
      "24        Trainable params\n",
      "0         Non-trainable params\n",
      "24        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214770f147d646f5bae6475c8be0c706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 198, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 737, in communicate_stop_status\n",
      "    resp = self._communicate(req, timeout=timeout, local=True)\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 539, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/yannik/Software/anaconda3/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 544, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "from HybridODE import NeuralODE_Hybrid\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "solver = 'dopri5'\n",
    "atol, rtol, atol_adjoint, rtol_adjoint = 1e-4,1e-4,1e-4,1e-4 # previously: 1e-4,1e-4,1e-8,1e-8\n",
    "model_Q = NeuralODE_Hybrid(f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint, IntegralLoss_Quadratic(f_Quadratic)).to(device) \n",
    "aug_model_Q = NeuralODE_Hybrid(aug_f_Quadratic, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint).to(device) \n",
    "learn_Q = EnergyShapingLearner(model_Q, t_span, prior, target, aug_model_Q).to(device) \n",
    "learn_Q.lr = 1e-3\n",
    "learn_Q.batch_size = 2\n",
    "logger_Q = WandbLogger(project='optimal-energy-shaping-SE3',name='quadratic-controller')\n",
    "trainer = pl.Trainer(max_epochs=3, logger=logger_Q)#\n",
    "trainer.fit(learn_Q)\n",
    "torch.save(f_Quadratic,'f_Quadratic.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop: Optimal Potential Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NeuralODE_Hybrid(f, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint, IntegralLoss(f))\n",
    "aug_model = NeuralODE_Hybrid(aug_f, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, atol_adjoint, rtol_adjoint)\n",
    "#x = torch.rand(1,13); x[:,12] = 0; x.requires_grad=True\n",
    "\n",
    "learn = EnergyShapingLearner(model, t_span, prior, target, aug_model)\n",
    "learn.lr = 5e-3\n",
    "learn.batch_size = 100\n",
    "\n",
    "logger = WandbLogger(project='optimal-energy-shaping-SE3', name='light_adjoint_raw')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger)#\n",
    "trainer.fit(learn)\n",
    "torch.save((I,B,V),'IBV_fShaping.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
