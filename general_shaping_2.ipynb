{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchdyn.numerics.odeint import odeint_hybrid\n",
    "from torchdyn.numerics.solvers import DormandPrince45\n",
    "import torchdyn.numerics.sensitivity\n",
    "import attr\n",
    "\n",
    "## lietorch:\n",
    "import sys; sys.path.append('../')\n",
    "import lie_torch as lie\n",
    "import math\n",
    "\n",
    "from functorch import vmap\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Only General Potential Shaping: gradients immediately on algebra, two potential terms on SE(3) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Definition: Mixed Dynamics on SE3 with NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import se3_Dynamics_2,SE3_Dynamics, SE3_Quadratic, AugmentedSE3, PosDefSym, PosDefTriv, PosDefSym_Small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity / Adjoint Method:\n",
    "\n",
    "This only needs the final condition of the forward dynamics and adjoint dynamics, then it computes both state and adjoint state from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensitivity import _gather_odefunc_hybrid_adjoint_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learners: Training step, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners import EnergyShapingLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event definition: Chart Switching on SE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from pytorch-implicit/exampels/network/simulate_tcp.ipynb, and Paper: Neural Hybrid Automata: Learning Dynamics withMultiple Modes and Stochastic Transitions (M.Poli, 2021)\n",
    "\n",
    "@attr.s\n",
    "class EventCallback(nn.Module):\n",
    "    def __attrs_post_init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def check_event(self, t, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def jump_map(self, t, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_jump(self, t, x, ev):\n",
    "        raise NotImplementedError\n",
    "\n",
    "@attr.s\n",
    "class ChartSwitch(EventCallback):       \n",
    "    def check_event(self, t, xi): \n",
    "        # works for collection of states\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        wi = qi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(wi,wi)) > math.pi*3/4).bool() \n",
    "        return ev\n",
    "\n",
    "    def jump_map(self, t, xi):\n",
    "        #xi = torch.squeeze(xi)\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = torch.unsqueeze(lie.bestChart(H),0)\n",
    "        qj = lie.chart_trans(qi, i, j)\n",
    "        return torch.cat((qj, P,j,rem), -1) #torch.unsqueeze(,0)\n",
    "    \n",
    "    def batch_jump(self, t, xi, ev):\n",
    "        xi[ev,:] = vmap(self.jump_map)(t[ev],xi[ev,:])\n",
    "        return xi\n",
    "        \n",
    "    def jump_map_forced(self, t, xi, j):\n",
    "        # jump to chart j for all\n",
    "        #xi = torch.squeeze(xi)\n",
    "        qi, P, i, rem = xi[...,:6], xi[...,6:12], xi[...,12], xi[...,13:]\n",
    "        qj = qi*0;\n",
    "        for k in range(qi.size(0)):\n",
    "            qj[k,:] = lie.chart_trans(qi[k,:], i[k], j[k])\n",
    "        return torch.cat((qj, P,torch.unsqueeze(j,1),rem), -1) #torch.unsqueeze(,0)\n",
    "    \n",
    "    def batch_jump_forced(self, t, xi, j):\n",
    "        xi = vmap(self.jump_map_forced)(t,xi,j)\n",
    "        return xi\n",
    "        \n",
    "    \n",
    "@attr.s\n",
    "class ChartSwitchAugmented(EventCallback):\n",
    "    des_props = None\n",
    "    # Expects x of type: z[:6] = qi, z[6:12] = P, z[12] = i, z[13:25] = λi, z[25:] = μ. This is used for the system augmented with co-state-dynamics for adjoint gradient method\n",
    "    def check_event(self, t, z): \n",
    "        xi, i, λi, rem = self.to_input(z)\n",
    "        w = xi[...,:3]\n",
    "        ev = (torch.sqrt(lie.dot(w,w)) < -1).bool()  \n",
    "        return ev\n",
    "\n",
    "    def batch_jump_to_Id(self, λiT, xT):\n",
    "        #only transitions λiT to corresponding version λT at identity of SE(3)\n",
    "        λT = vmap(lie.dchart_trans_mix_Co_to_Id)(λiT,xT[...,:12],xT[...,-1])\n",
    "        return λT\n",
    "    \n",
    "    def jump_map(self, t, z):\n",
    "        xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "        qi = xi[...,:6]\n",
    "        H = lie.unchart(qi, i)\n",
    "        j = lie.bestChart(H)\n",
    "        xj = lie.chart_trans_mix(xi, i, j)\n",
    "        λj = lie.chart_trans_mix_Co(xi, λi, i, j)\n",
    "        return torch.cat((xj, torch.unsqueeze(j,-1), λj, rem), -1)\n",
    "    \n",
    "    def batch_jump(self, t, z, ev):\n",
    "        xi, i, λii, rem = self.to_input(z)\n",
    "        z = torch.cat((xi,torch.unsqueeze(i,-1),λii),-1)\n",
    "        z[ev,:] = vmap(self.jump_map)(t[:xi.shape[0]][ev],z[ev,:])\n",
    "        xi, i, λii = z[...,:12], z[...,12], z[...,13:26] \n",
    "        return self.to_output(xi, i, λii, rem)\n",
    "    \n",
    "    def to_input(self, z):\n",
    "        if (self.des_props!=None):\n",
    "            numels,shapes = tuple(self.des_props)\n",
    "            xii_nel, λi_nel = tuple(numels)\n",
    "            xii_shp, λi_shp = tuple(shapes)\n",
    "            xii, λii, rem = z[:xii_nel], z[xii_nel:xii_nel+λi_nel], z[xii_nel+λi_nel:]\n",
    "            xii, λii = xii.reshape(xii_shp), λii.reshape(λi_shp)\n",
    "            xi, i = xii[...,:12], torch.unsqueeze(xii[...,12],-1)\n",
    "            return xi, i, λii, rem\n",
    "        else:\n",
    "            xi, i, λi, rem  = z[...,:12], z[...,12], z[...,13:25], z[...,25:]\n",
    "            return xi, i, λi, rem\n",
    "    \n",
    "    def to_output(self, xj, j, λj, rem):\n",
    "        if (self.des_props!= None):\n",
    "            xjj = torch.cat((xj,torch.unsqueeze(j,-1)),-1)\n",
    "            z = torch.cat((xjj.flatten(),λj.flatten(),rem))\n",
    "        else:\n",
    "            z = torch.cat((xj, torch.unsqueeze(j,-1), λj, rem), -1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of Dynamics, Definition of Loss-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from latent-energy-shaping-main/notebooks/optimal_energy_shaping.ipynb\n",
    "\n",
    "I = torch.diag(torch.tensor((0.01,0.01,0.01,1,1,1))).to(device) ; # Inertia Tensor\n",
    "\n",
    "from models import IntegralLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definition of NNs for potential and damping injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 32\n",
    "\n",
    "V1 = nn.Sequential(nn.Linear(12, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1)).to(device)\n",
    "V2 = nn.Sequential(nn.Linear(3, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, 1)).to(device)\n",
    "\n",
    "### Likewise for Damping injection:\n",
    "nf = 6\n",
    "B = nn.Sequential(nn.Linear(18, nh), nn.Softplus(), nn.Linear(nh, nh), nn.Tanh(), nn.Linear(nh, nf),PosDefTriv()).to(device)\n",
    "\n",
    "### Initialize Parameters: \n",
    "\n",
    "for p in V1.parameters(): torch.nn.init.normal_(p, mean=0.0, std=0.01)#torch.nn.init.zeros_(p)\n",
    "for p in V2.parameters(): torch.nn.init.normal_(p, mean=0.0, std=0.01)\n",
    "for p in B.parameters(): torch.nn.init.normal_(p, mean=0.0, std=0.01) #torch.nn.init.zeros_(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of Chart-Switches, Prior- \\& Target Distribution, and Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prior and Target Distribution\n",
    "\n",
    "from utils import prior_dist_SE3, target_dist_SE3_HP,  target_cost_SE3, multinormal_target_dist\n",
    "\n",
    "th_max = torch.tensor(math.pi).to(device); d_max = torch.tensor(0).to(device); pw_max = torch.tensor(0.0).to(device); pv_max = torch.tensor(1.0).to(device); ch_min = torch.tensor(0).to(device); ch_max = torch.tensor(0).to(device); \n",
    "prior = prior_dist_SE3(th_max,d_max,pw_max,pv_max,ch_min,ch_max,device)\n",
    "\n",
    "H_target = torch.eye(4).to(device); sigma_th = torch.tensor(4).to(device); sigma_d = torch.tensor(10).to(device); sigma_pw = torch.tensor(5).to(device); sigma_p = torch.tensor(1).to(device);\n",
    "                                    # sigma_th = torch.tensor(0.4).to(device); sigma_d = torch.tensor(0.4).to(device); sigma_pw = torch.tensor(1e-1).to(device); sigma_p = torch.tensor(1e-1).to(device);\n",
    "target = target_cost_SE3(H_target,sigma_th,sigma_d,sigma_pw,sigma_p,device)\n",
    "\n",
    "integral_loss_scale = torch.tensor(0.01).to(device)\n",
    "\n",
    "### Callback:\n",
    "callbacks = [ChartSwitch()]\n",
    "jspan = 10 # maximum number of chart switches per iteration (if this many happen, something is wrong anyhow)\n",
    "\n",
    "callbacks_adjoint = [ChartSwitchAugmented()]\n",
    "jspan_adjoint = 10\n",
    "\n",
    "### Initialize Dynamics \n",
    "#(I,B,V1,V2) = torch.load('IBV_fShaping_07_07_11:57.pt') #('IBV_fShaping_19_09_16:25.pt')\n",
    "I = I.to(device); B = B.to(device); V1 = V1.to(device); V2 = V2.to(device)\n",
    "f = se3_Dynamics_2(I,B,V1,V2,target).to(device) # (I,B,V) = torch.load('IBV_fShaping.pt')\n",
    "\n",
    "## Augmented Dynamics with integral loss\n",
    "aug_f = AugmentedSE3(f, IntegralLoss(f),target).to(device) \n",
    "aug_f.l.scale = integral_loss_scale\n",
    "\n",
    "t_span = torch.linspace(0, 3, 30).to(device)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop: Optimal Potential Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.parameters()\n",
    "#model_parameters = filter(lambda p: p.requires_grad, f.parameters())\n",
    "#params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "#print(params)\n",
    "#model_parameters = filter(lambda p: p.requires_grad, f.parameters())\n",
    "\n",
    "#[print(p) for p in model_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mypwotte\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yannik/Documents/Python/PhD/Optimal Potential Shaping/Code/V0/wandb/run-20230713_160641-3e9e1d9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ypwotte/potential-shaping-SE3/runs/3e9e1d9q\" target=\"_blank\">NN_13/07_16:06</a></strong> to <a href=\"https://wandb.ai/ypwotte/potential-shaping-SE3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | NeuralODE_Hybrid | 4.6 K \n",
      "1 | aug_model | NeuralODE_Hybrid | 4.6 K \n",
      "-----------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1938: PossibleUserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b8f12418104a6bb649a8df96cb338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/yannik/Programs/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from HybridODE import NeuralODE_Hybrid\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import datetime\n",
    "\n",
    "today = datetime. datetime. now()\n",
    "date_time = today. strftime(\"%d/%m_%H:%M\")\n",
    "\n",
    "solver = 'dopri5'\n",
    "atol, rtol, atol_adjoint, rtol_adjoint = 1e-3,1e-4,1e-3,1e-4\n",
    "dt_min, dt_min_adjoint = 0, 0\n",
    "\n",
    "model = NeuralODE_Hybrid(f, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, dt_min, atol_adjoint, rtol_adjoint, dt_min_adjoint, IntegralLoss(f), sensitivity = 'hybrid_adjoint_full').to(device) \n",
    "aug_model = NeuralODE_Hybrid(aug_f, jspan, callbacks, jspan_adjoint, callbacks_adjoint, solver, atol, rtol, dt_min, atol_adjoint, rtol_adjoint, dt_min_adjoint, sensitivity = 'hybrid_adjoint_full').to(device) \n",
    "\n",
    "learn = EnergyShapingLearner(model, t_span, prior, target, aug_model).to(device) \n",
    "learn.lr = 1e-3\n",
    "learn.batch_size = 2048\n",
    "\n",
    "logger = WandbLogger(project='potential-shaping-SE3', name='NN_'+date_time)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=150, logger=logger, gpus = torch.cuda.device_count())#\n",
    "trainer.fit(learn)\n",
    "#torch.save((I,B,V),'IBV_fShaping_'+date_time+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = today.strftime(\"%d_%m_%H:%M\")\n",
    "torch.save((I,B,V1,V2),'IBV_fShaping_'+date_time+'.pt')\n",
    "\n",
    "#%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
